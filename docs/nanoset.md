# Nanoset

## Data pre-processing

Nanotron incorporates [`Nanosets`](../src/nanotron/data/nanoset.py), a streamlined version of Megatron-LM datasets. It also includes [`BlendedNanosets`](../src/nanotron/data/blended_nanoset.py), to be able to combine different Nanosets.

The training data requires preprocessing. Just like Megatron, first, place your training data in a loose json format, with one json containing a text sample per line. For example:

<pre>
{"src": "www.nvidia.com", "text": "The quick brown fox", "type": "Eng", "id": "0", "title": "First Part"}
{"src": "The Internet", "text": "jumps over the lazy dog", "type": "Eng", "id": "42", "title": "Second Part"}
</pre>

The loose json is then processed into a binary format for training using the [`tools/preprocess_data.py`](../tools/preprocess_data.py) script. Below we show an example for processing a corpus with the Llama2 tokenizer.

<pre>
python tools/preprocess_data.py \
       --input data/my_corpus.json \
       --output-prefix data/processed-datasets/my-llama2-dataset \
       --pretrained-model-name-or-path models/Llama-2-7b \
       --workers 128 \
       --partitions 8
</pre>

In `--pretrained-model-name-or-path`, we will have to specify a tokenizer in the same way as we do when using `AutoTokenizers.from_pretrained(...)`.

The output will be two files named, in this case, `my-llama2-dataset_text.bin` and `my-llama2-dataset_text.idx`. The `data_path` specified later in training is the full path and new filename, but **without** the file extension.

We also include the following scripts:
- [`hf_datasets_to_json.py`](../tools/hf_datasets_to_json.py): A tool to transform datasets from the Hugging Face Hub (or local) to the .json format required by `preprocess_data.py`.
- [`merge_datasets.py`](../tools/merge_datasets.py): A tool to merge preprocessed datasets (_.bin_ and _.idx_ files) into a single file.

## Working with Nanosets

To work with Nanosets, we need to configure 3 arguments:
1. `split`: The distribution we want to divide our dataset into among train, valid, and test split.
2. `path_to_cache`: Directory used to store certain metadata of Nanosets to reuse them between different runs.
3. `data_path`: This argument specifies the file/files that will compose the Nanoset. There are 2 ways to specify it:
   1. If we only indicate _one_ path (path to the files generated by preprocess_data.py WITHOUT the extension), we will create a `Nanoset`.
    ```yaml
    data:
      dataset:
        data_path: nanoset/SlimPajama-6B_text
        split: 949,50,1
        path_to_cache: .nanoset_cache
      num_loading_workers: 0
      seed: 1234
    ```
   2. With a dictionary, we can create a `BlendedNanoset` where the keys are the paths to the dataset (path to the files generated by preprocess_data.py WITHOUT the extension) and the values are the weight for each dataset.
    ```yaml
    data:
      dataset:
        data_path:
          nanoset/SlimPajama-6B_text: 0.8
          nanoset/europarl_text: 0.2
        split: 949,50,1
        path_to_cache: .nanoset_cache
      num_loading_workers: 0
      seed: 1234
    ```

Finally, to use the Nanosets, launch the training with [`run_train_nanoset.py`](../run_train_nanoset.py).
```shell
torchrun --nproc-per-node 8 run_train_nanoset.py --config configs/nanoset_llama2.yaml
```

## Under the hood
### Number of samples

When using Nanosets, we specify the `data_path` to the preprocessed dataset and a `split`. This `split` value will be used to divide the total number of documents into train, valid, and test sets.

For the train split, the number of samples consumed from the Nanoset will be determined by the `number of train steps * global batch size`, so if this number is higher than the number of samples created with the documents assigned to the train split, we will see the dataset samples more than once (> 1 epoch). In the case of the valid and test split, we will see all the samples only once.

In the case of the `BlendedNanoset`, we will also indicate the weight of each dataset to construct data batches according to the specified proportion. In this case, the train split will respect this proportion, considering that the number of samples will be calculated in the same way as in the Nanosets, so it may happen that we consume one dataset for 3 epochs and another larger dataset for only one epoch. For the valid and test splits, the same as in the Nanosets will occur; we will consume all the samples only once.

### Nanoset
A `Nanoset` is paremeterized by the following variables:
- The underlying `MMapIndexedDataset` instance (`indexed_dataset`)
- The sequence length `S`
- The split indices `indexed_indices` (the congituous subset of document or sequence indices used for training, validation, and testing)
- The total number of samples `N` of the Nanoset that we will consume during training. In the case of the valid and test splits, we will only consume the dataset once
- The random seed `R`

The `Nanoset` creates three index mappings to facilitate lookup: (1) The `document_index`, (2) the `sample_index`, and (3) the `shuffle_index`.

1. The document index (`document_index`) is a 1-D array array in which the indices _i_ contain the indices of the documents (**each sample from the JSON file**) that belong to the dataset. This array is randomly shuffled with seed `R`.
```
Given:
len(indexed_dataset) => 40

indexed_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]

Then, for example:

document_index = [14, 8, 17, 0, 3, 15, 7, 19, 11, 12, 9, 2, 10, 1, 4, 16, 13, 5, 18, 6]
```
2. The sample index (`sample_index`) is a 2-D array mapping from _j_ to pairs of (_i_, document_index[_i_] offset) of shape `[num_samples + 1, 2]`, where `num_samples` is `(tokens_per_epoch - 1) // S`. The rows _j_ and _j + 1_ serve as the left and right bounds for the j-th sample, so that each sample will contain `S` tokens.
```
Given:

S = 1024

Then, for example:

sample_index[0] = (0, 0)
sample_index[1] = (0, 1024)       => document_index[0] (Document 14) has length greater than S
sample_index[2] = (1, 512)        => document_index[0] has length 1536
sample_index[3] = (2, 100)        => document_index[1] (Document 8) has length 1436 (In this sample we get 924 from document_index[1] + 100 from document_index[2])
sample_index[4] = (3, 0)          => document_index[2] has length 1124
sample_index[5] = (5, 300)        => document_index[3:5] have a total length of 724 (S - 300)
sample_index[6] = (6, 24)         => document_index[5] has length 1300
...
sample_index[15] = (18, 512)
sample_index[16] = (19, 1000)     => From document_index[19] we don't have S tokens, so this sample is discarded

len(sample_index) => 17, but only 16 useful samples
```
3. In the train split, the shuffle index (`shuffle_index`) is a 1-D array mapping from _k_ to _j_ of length `n_concatenations * (len(sample_index) - 1)`, where `n_concatenations` is defined as `(N / (len(sample_index) - 1)) + 1`, so that `len(shuffle_index)` is always greater than `N`. While for the valid and test splits, `len(shuffle_index) == len(sample_index) - 1`. Before concatenating the full array, `shuffle_index` is shuffled according to `R`.
```
Given:

N = 70

Then, for example:

shuffle_index = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

Shuffle the indices -> shuffle_index = [9, 3, 15, 12, 5, 10, 1, 4, 8, 11, 13, 7, 2, 14, 6, 0]

n_concatenations = (70/(17 - 1)) + 1 = 5
shuffle_index = shuffle_index concatenated 5 times

len(shuffle_index) = 80 > N
```

To query the `Nanoset` for the k-th sample we do the following:
1. Use the `shuffle_index` to get the index _j_ into the `sample_index`
```
j = shuffle_index[k]
```
2. Use the `sample_index` to get the left and right sample-bounding indices into the `document_index` and the starting token offset for each document
```
i, offset = sample_index[j]
i_next, offset_next = sample_index[j + 1]
```
3. Use the `document_index` to retrieve `S + 1` tokens from consecutive (in the `document_index`) documents
```
sample = []
sample += indexed_dataset[document_index[i]][offset:]
if i != i_next:
    sample += indexed_dataset[document_index[i + 1:i_next]]
sample += indexed_dataset[document_index[i_next]][:offset_next]
```

Despite having repeated indices in the `shuffle_index`, throughout 1 epoch, we will only observe each sample once. We achieve this by deactivating shuffling in the `DistributedDataSampler`, so that the indices of the `shuffle_index` are consumed in the order they appear by the multiple processes. It is worth noting that the `document_index` already shuffles the data so that the samples constructed in the `sample_index` do not contain data from consecutive documents, and we shuffle the data again in the `shuffle_index`.
```
Given:

4 Processes loading data

(P1) idx_list = [0, 4, 8, 12, 16, ...]    -> shuffle_index[idx_list] = [9, 5, 8, 2, 9, ...]
(P2) idx_list = [1, 5, 9, 13, 17, ...]    -> shuffle_index[idx_list] = [3, 10, 11, 14, 3, ...]
(P3) idx_list = [2, 6, 10, 14, 18, ...]   -> shuffle_index[idx_list] = [15, 1, 13, 6, 15, ...]
(P4) idx_list = [3, 7, 11, 15, 19, ...]   -> shuffle_index[idx_list] = [12, 4, 7, 0, 12, ...]
```
### BlendedNanoset
The `BlendedNanoset` is parameterized by the following variables:
- The underlying `Nanoset` instances `D`
- The weights `W` (one per dataset)
- The number of samples `U`

The `BlendedNanoset` creates two "blending" indices to facilitate lookup: (1) The `dataset_index` and (2) the `dataset_sample_index`.

1. The `dataset_index` is a 1-D array mapping from _i_ to dataset index from `D` of length `U`.
```
Given:

D = [d0, d1, d2, d3]
W = [0.1, 0.5, 0.3, 0.1]
U = 20

Then, for example:

dataset_index = [1, 2, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 3, 1, 2, 1, 2, 1]
```
2. The `dataset_sample_index` is a 1-D mapping from _i_ to the sample index for dataset_index[_i_] of length `U`.
```
dataset_index =         [1, 2, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 3, 1, 2, 1, 2, 1]
dataset_sample_index =  [0, 0, 0, 1, 0, 2, 1, 3, 2, 4, 1, 5, 3, 6, 1, 7, 4, 8, 5, 9]
```
To query the `BlendedNanoset` for the k-th sample we do the following:
- Use the `dataset_index` to retrieve the corresponding dataset from `D` and the `dataset_sample_index` to retrieve the corresponding sample from that dataset.
```
sample = D[dataset_index[k]][dataset_sample_index[k]]
```
