# Nanoset

## Data pre-processing

Nanotron incorporates [`Nanosets`](../src/nanotron/data/nanoset.py), a streamlined version of Megatron-LM datasets. It also includes [`BlendedNanosets`](../src/nanotron/data/blended_nanoset.py), to be able to combine different Nanosets.

The training data requires preprocessing. Just like Megatron, first, place your training data in a loose json format, with one json containing a text sample per line. For example:

<pre>
{"src": "www.nvidia.com", "text": "The quick brown fox", "type": "Eng", "id": "0", "title": "First Part"}
{"src": "The Internet", "text": "jumps over the lazy dog", "type": "Eng", "id": "42", "title": "Second Part"}
</pre>

The loose json is then processed into a binary format for training using the [`tools/preprocess_data.py`](../tools/preprocess_data.py) script. Below we show an example for processing a corpus with the Llama2 tokenizer.

<pre>
python tools/preprocess_data.py \
       --input data/my_corpus.json \
       --output-prefix data/processed-datasets/my-llama2-dataset \
       --pretrained-model-name-or-path models/Llama-2-7b \
       --workers 128 \
       --partitions 8
</pre>

In `--pretrained-model-name-or-path`, we will have to specify a tokenizer in the same way as we do when using `AutoTokenizers.from_pretrained(...)`.

The output will be two files named, in this case, `my-llama2-dataset_text.bin` and `my-llama2-dataset_text.idx`. The `data_path` specified later in training is the full path and new filename, but **without** the file extension.

We also include the following scripts:
- [`hf_datasets_to_json.py`](../tools/hf_datasets_to_json.py): A tool to transform datasets from the Hugging Face Hub (or local) to the .json format required by `preprocess_data.py`.
- [`merge_datasets.py`](../tools/merge_datasets.py): A tool to merge preprocessed datasets (_.bin_ and _.idx_ files) into a single file.

## NanosetDatasetsArgs

To work with Nanosets, we need to configure 3 arguments:
1. `split`: The distribution we want to divide our dataset into among train, valid, and test split.
2. `path_to_cache`: Directory used to store certain metadata of Nanosets to reuse them between different runs.
3. `data_path`: This argument specifies the file/files that will compose the Nanoset. There are 2 ways to specify it:
   1. If we only indicate _one_ path (path to the files generated by preprocess_data.py WITHOUT the extension), we will create a Nanoset.
    ```yaml
    data:
      dataset:
        data_path: nanoset/SlimPajama-6B_text
        split: 949,50,1
        path_to_cache: .nanoset_cache
      num_loading_workers: 0
      seed: 1234
    ```
   2. With a dictionary, we can create a BlendedNanoset where the keys are the paths to the dataset (path to the files generated by preprocess_data.py WITHOUT the extension) and the values are the weight for each dataset.
    ```yaml
    data:
      dataset:
        data_path:
          nanoset/SlimPajama-6B_text: 0.8
          nanoset/europarl_text: 0.2
        split: 949,50,1
        path_to_cache: .nanoset_cache
      num_loading_workers: 0
      seed: 1234
    ```

Finally, to use the Nanosets, launch the training with [`run_train_nanoset.py`](../run_train_nanoset.py).
```shell
torchrun --nproc-per-node 8 run_train_nanoset.py --config configs/nanoset_llama2.yaml
```
