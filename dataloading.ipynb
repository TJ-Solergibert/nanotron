{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = '127.0.0.1'\n",
    "os.environ[\"MASTER_PORT\"] = '29500'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"/mloscratch/homes/solergib/s-ai/nanotron/examples/debug_llama_1GPU_dummy.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nanotron import logging\n",
    "from nanotron.config import (\n",
    "    PretrainDatasetsArgs,\n",
    ")\n",
    "from nanotron.dataloader import (\n",
    "    clm_process,\n",
    "    dummy_infinite_data_generator,\n",
    "    get_datasets,\n",
    "    get_train_dataloader,\n",
    ")\n",
    "from nanotron.logging import log_rank\n",
    "from nanotron.parallel.pipeline_parallel.utils import get_input_output_pp_ranks\n",
    "from nanotron.trainer import DistributedTrainer\n",
    "from nanotron.utils import (\n",
    "    main_rank_first,\n",
    ")\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import __version__ as hf_hub_version\n",
    "    from transformers import AutoTokenizer\n",
    "    from transformers import __version__ as tf_version\n",
    "except ImportError:\n",
    "    hf_hub_version = None\n",
    "    tf_version = None\n",
    "\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]: Setting max_position_embeddings to 4096. Previous value was 2048.\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]: Config:\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]: Config(general=GeneralArgs(project='debug',\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                            entity=None,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                            run='llama',\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                            seed=42,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                            step=None,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                            consumed_train_samples=None,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                            benchmark_csv_path=None,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                            ignore_sanity_checks=True),\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:        parallelism=ParallelismArgs(dp=1,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                    pp=1,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                    tp=1,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                    pp_engine=<nanotron.parallel.pipeline_parallel.engine.OneForwardOneBackwardPipelineEngine object at 0x7f2a6f2aabf0>,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                    tp_mode=<TensorParallelLinearMode.REDUCE_SCATTER: 2>,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                    tp_linear_async_communication=True,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                    expert_parallel_size=1),\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:        model=ModelArgs(model_config=LlamaConfig(bos_token_id=1,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 eos_token_id=2,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 hidden_act='silu',\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 hidden_size=4096,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 initializer_range=0.02,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 intermediate_size=11008,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 is_llama_config=True,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 max_position_embeddings=4096,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 num_attention_heads=16,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 num_hidden_layers=16,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 num_key_value_heads=16,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 pad_token_id=None,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 pretraining_tp=1,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 rms_norm_eps=1e-06,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 rope_scaling=None,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 tie_word_embeddings=False,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 use_cache=True,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                 vocab_size=32000),\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                        init_method=RandomInit(std=0.025),\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                        dtype=torch.bfloat16,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                        make_vocab_size_divisible_by=1,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                        ddp_bucket_cap_mb=25),\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:        tokenizer=TokenizerArgs(tokenizer_name_or_path='/mloscratch/homes/solergib/models/Llama-2-7b-chat-hf',\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                tokenizer_revision=None,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                tokenizer_max_length=None),\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:        checkpoints=CheckpointsArgs(checkpoints_path=PosixPath('/mloscratch/homes/solergib/s-ai/nanotron/checkpoints'),\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                    checkpoint_interval=1000,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                    save_initial_state=False,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                    resume_checkpoint_path=None,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                    checkpoints_path_is_shared_file_system=False),\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:        logging=LoggingArgs(log_level='info',\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                            log_level_replica='info',\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                            iteration_step_info_interval=1),\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:        tokens=TokensArgs(sequence_length=4096,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                          train_steps=50,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                          micro_batch_size=4,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                          batch_accumulation_per_replica=8,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                          val_check_interval=-1,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                          limit_val_batches=0,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                          limit_test_batches=0),\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:        optimizer=OptimizerArgs(zero_stage=0,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                weight_decay=0.01,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                clip_grad=1.0,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                accumulate_grad_in_fp32=True,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                adam_eps=1e-08,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                adam_beta1=0.9,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                adam_beta2=0.95,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                torch_adam_is_fused=True,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                learning_rate_scheduler=LRSchedulerArgs(learning_rate=0.0003,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                                        lr_warmup_steps=2,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                                        lr_warmup_style='linear',\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                                        lr_decay_style='cosine',\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                                        lr_decay_steps=3,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                                        lr_decay_starting_step=None,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                                        min_decay_lr=1e-05)),\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:        data=DataArgs(dataset=PretrainDatasetsArgs(hf_dataset_or_datasets='stas/openwebtext-10k',\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                   hf_dataset_splits='train',\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                   hf_dataset_config_name=None,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                   dataset_processing_num_proc_per_process=1,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                   dataset_overwrite_cache=False,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                                                   text_column_name='text'),\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                      seed=42,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:                      num_loading_workers=1),\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:        profiler=None,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:        lighteval=None)\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]: Model Config:\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]: LlamaConfig(bos_token_id=1,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             eos_token_id=2,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             hidden_act='silu',\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             hidden_size=4096,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             initializer_range=0.02,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             intermediate_size=11008,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             is_llama_config=True,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             max_position_embeddings=4096,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             num_attention_heads=16,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             num_hidden_layers=16,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             num_key_value_heads=16,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             pad_token_id=None,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             pretraining_tp=1,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             rms_norm_eps=1e-06,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             rope_scaling=None,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             tie_word_embeddings=False,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             use_cache=True,\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]:             vocab_size=32000)\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]: Building model..\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]: Setting PP block ranks..\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]: Total number of parameters: 3.5G (6676.26MiB)\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]: Local number of parameters: 3.5G (6676.26MiB)\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]: [After model building] Memory usage: 6740.28MiB. Peak allocated: 6748.31MiB Peak reserved: 6758.00MiB\n",
      "03/09/2024 21:35:59 [INFO|DP=0|PP=0|TP=0]: No checkpoint path provided.\n"
     ]
    }
   ],
   "source": [
    "trainer = DistributedTrainer(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/09/2024 21:36:00 [INFO|DP=0|PP=0|TP=0]: Using `datasets` library\n",
      "03/09/2024 21:36:00 [INFO|DP=0|PP=0|TP=0]: Loading tokenizer from /mloscratch/homes/solergib/models/Llama-2-7b-chat-hf and transformers/hf_hub versions ('4.37.2', '0.20.3')\n"
     ]
    }
   ],
   "source": [
    "# OG Dataloader\n",
    "def get_dataloader(trainer: DistributedTrainer):\n",
    "    \"\"\"Returns a dataloader for training.\"\"\"\n",
    "\n",
    "    # First, we need to know which ranks to feed the dataloader to\n",
    "    input_pp_rank, output_pp_rank = get_input_output_pp_ranks(model=trainer.model)\n",
    "\n",
    "    # Case 1: Dummy data generator\n",
    "    if trainer.config.data.dataset is None:\n",
    "        log_rank(\"Using dummy data generator\", logger=logger, level=logging.INFO, rank=0)\n",
    "        dataloader = dummy_infinite_data_generator(\n",
    "            micro_batch_size=trainer.micro_batch_size,\n",
    "            sequence_length=trainer.sequence_length,\n",
    "            input_pp_rank=input_pp_rank,\n",
    "            output_pp_rank=output_pp_rank,\n",
    "            vocab_size=trainer.model_config.vocab_size,\n",
    "            seed=trainer.config.data.seed,\n",
    "            parallel_context=trainer.parallel_context,\n",
    "        )()\n",
    "\n",
    "    # Case 2: HuggingFace datasets\n",
    "    elif isinstance(trainer.config.data.dataset, PretrainDatasetsArgs):\n",
    "        log_rank(\"Using `datasets` library\", logger=logger, level=logging.INFO, rank=0)\n",
    "        tokenizer_path = trainer.config.tokenizer.tokenizer_name_or_path\n",
    "        log_rank(\n",
    "            f\"Loading tokenizer from {tokenizer_path} and transformers/hf_hub versions {tf_version, hf_hub_version}\",\n",
    "            logger=logger,\n",
    "            level=logging.INFO,\n",
    "            rank=0,\n",
    "        )\n",
    "\n",
    "        # We need to the 1st device to process dataset and cache it, then other devices load from cache\n",
    "        with main_rank_first(trainer.parallel_context.world_pg):\n",
    "            # TODO @nouamanetazi: this may timeout before 1st device finishes processing dataset. Can we have a ctxmanager to modify timeout?\n",
    "            # TODO: generalise to include  for validation/test splits\n",
    "\n",
    "            # We load the raw dataset\n",
    "            raw_dataset = get_datasets(\n",
    "                hf_dataset_or_datasets=trainer.config.data.dataset.hf_dataset_or_datasets,\n",
    "                splits=trainer.config.data.dataset.hf_dataset_splits,\n",
    "            )[\"train\"]\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.padding_side = \"left\"\n",
    "\n",
    "            # We apply the Causal Language Modeling preprocessing\n",
    "            train_dataset = clm_process(\n",
    "                raw_dataset=raw_dataset,\n",
    "                tokenizer=tokenizer,\n",
    "                text_column_name=trainer.config.data.dataset.text_column_name,\n",
    "                dataset_processing_num_proc_per_process=trainer.config.data.dataset.dataset_processing_num_proc_per_process,\n",
    "                dataset_overwrite_cache=trainer.config.data.dataset.dataset_overwrite_cache,\n",
    "                sequence_length=trainer.sequence_length,\n",
    "            )\n",
    "\n",
    "            # We load the processed dataset on the ranks requiring it\n",
    "            dataloader = get_train_dataloader(\n",
    "                train_dataset=train_dataset,\n",
    "                sequence_length=trainer.sequence_length,\n",
    "                parallel_context=trainer.parallel_context,\n",
    "                input_pp_rank=input_pp_rank,\n",
    "                output_pp_rank=output_pp_rank,\n",
    "                micro_batch_size=trainer.micro_batch_size,\n",
    "                consumed_train_samples=trainer.consumed_train_samples,\n",
    "                dataloader_num_workers=trainer.config.data.num_loading_workers,\n",
    "                seed_worker=trainer.config.data.seed,\n",
    "                dataloader_drop_last=True,\n",
    "            )\n",
    "            # Check if we have enough samples for train_steps\n",
    "            total_tokens_dataset = len(dataloader.dataset) * trainer.sequence_length\n",
    "            num_tokens_needed_for_training = (\n",
    "                (trainer.config.tokens.train_steps - trainer.start_iteration_step)\n",
    "                * trainer.global_batch_size\n",
    "                * trainer.sequence_length\n",
    "            )\n",
    "            assert num_tokens_needed_for_training <= total_tokens_dataset, (\n",
    "                f\"Dataset is too small for steps ({total_tokens_dataset} < {num_tokens_needed_for_training}), \"\n",
    "                f\"Try train_steps<={len(dataloader.dataset) // trainer.global_batch_size + trainer.start_iteration_step}\"\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"Unhandled case of `self.config.data.dataset`. Got: {trainer.config.data.dataset}\")\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "dataloader = get_dataloader(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En training_step hace \n",
    "n_micro_batches_per_batch = 2\n",
    "batch=(next(dataloader) for _ in range(n_micro_batches_per_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luego en OneForwardOneBackwardPipelineEngine.train_batch_iter hace\n",
    "batch_iter = iter(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch_iter_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# En training_step hace \u001b[39;00m\n\u001b[1;32m      2\u001b[0m n_micro_batches_per_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 3\u001b[0m batch\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_micro_batches_per_batch))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not an iterator"
     ]
    }
   ],
   "source": [
    "batch_iter_next = next(batch_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for micro_batch in batch_iter:\n",
    "    print('222')\n",
    "    print(micro_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  297,  3122, 29889,  ...,  7432,   338,   451],\n",
      "        [ 7047, 29892,   470,  ..., 27477,   839,   263],\n",
      "        [  813,  3802,   394,  ..., 29879, 29892,   338],\n",
      "        [13630,   293, 27881,  ...,   393,   723, 29548]]), 'input_mask': tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]]), 'label_ids': tensor([[ 3122, 29889,    13,  ...,   338,   451,  3307],\n",
      "        [29892,   470,   491,  ...,   839,   263,   365],\n",
      "        [ 3802,   394,  3687,  ..., 29892,   338,   263],\n",
      "        [  293, 27881,   310,  ...,   723, 29548,   470]]), 'label_mask': tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])}\n"
     ]
    }
   ],
   "source": [
    "for data in dataloader: # Imprime con microbatchsize 4 el resultado que queremos ver dentro del engine. NO conseguimos replicar lo del iter y los next que hace, ni idea, pero pasamos a megatron\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 3097\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Megatron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]: Setting max_position_embeddings to 4096. Previous value was 2048.\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]: Config:\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]: Config(general=GeneralArgs(project='debug',\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                            entity=None,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                            run='llama',\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                            seed=42,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                            step=None,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                            consumed_train_samples=None,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                            benchmark_csv_path=None,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                            ignore_sanity_checks=True),\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:        parallelism=ParallelismArgs(dp=1,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                    pp=1,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                    tp=1,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                    pp_engine=<nanotron.parallel.pipeline_parallel.engine.OneForwardOneBackwardPipelineEngine object at 0x7f924d093610>,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                    tp_mode=<TensorParallelLinearMode.REDUCE_SCATTER: 2>,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                    tp_linear_async_communication=True,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                    expert_parallel_size=1),\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:        model=ModelArgs(model_config=LlamaConfig(bos_token_id=1,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 eos_token_id=2,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 hidden_act='silu',\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 hidden_size=4096,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 initializer_range=0.02,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 intermediate_size=11008,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 is_llama_config=True,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 max_position_embeddings=4096,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 num_attention_heads=16,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 num_hidden_layers=16,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 num_key_value_heads=16,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 pad_token_id=None,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 pretraining_tp=1,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 rms_norm_eps=1e-06,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 rope_scaling=None,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 tie_word_embeddings=False,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 use_cache=True,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                 vocab_size=32000),\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                        init_method=RandomInit(std=0.025),\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                        dtype=torch.bfloat16,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                        make_vocab_size_divisible_by=1,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                        ddp_bucket_cap_mb=25),\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:        tokenizer=TokenizerArgs(tokenizer_name_or_path='/mloscratch/homes/solergib/models/Llama-2-7b-chat-hf',\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                tokenizer_revision=None,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                tokenizer_max_length=None),\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:        checkpoints=CheckpointsArgs(checkpoints_path=PosixPath('/mloscratch/homes/solergib/s-ai/nanotron/checkpoints'),\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                    checkpoint_interval=1000,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                    save_initial_state=False,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                    resume_checkpoint_path=None,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                    checkpoints_path_is_shared_file_system=False),\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:        logging=LoggingArgs(log_level='info',\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                            log_level_replica='info',\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                            iteration_step_info_interval=1),\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:        tokens=TokensArgs(sequence_length=4096,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                          train_steps=50,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                          micro_batch_size=4,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                          batch_accumulation_per_replica=8,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                          val_check_interval=-1,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                          limit_val_batches=0,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                          limit_test_batches=0),\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:        optimizer=OptimizerArgs(zero_stage=0,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                weight_decay=0.01,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                clip_grad=1.0,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                accumulate_grad_in_fp32=True,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                adam_eps=1e-08,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                adam_beta1=0.9,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                adam_beta2=0.95,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                torch_adam_is_fused=True,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                learning_rate_scheduler=LRSchedulerArgs(learning_rate=0.0003,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                                        lr_warmup_steps=2,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                                        lr_warmup_style='linear',\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                                        lr_decay_style='cosine',\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                                        lr_decay_steps=3,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                                        lr_decay_starting_step=None,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                                        min_decay_lr=1e-05)),\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:        data=DataArgs(dataset=PretrainDatasetsArgs(hf_dataset_or_datasets='stas/openwebtext-10k',\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                   hf_dataset_splits='train',\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                   hf_dataset_config_name=None,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                   dataset_processing_num_proc_per_process=1,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                   dataset_overwrite_cache=False,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                                                   text_column_name='text'),\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                      seed=42,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:                      num_loading_workers=1),\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:        profiler=None,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:        lighteval=None)\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]: Model Config:\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]: LlamaConfig(bos_token_id=1,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             eos_token_id=2,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             hidden_act='silu',\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             hidden_size=4096,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             initializer_range=0.02,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             intermediate_size=11008,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             is_llama_config=True,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             max_position_embeddings=4096,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             num_attention_heads=16,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             num_hidden_layers=16,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             num_key_value_heads=16,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             pad_token_id=None,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             pretraining_tp=1,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             rms_norm_eps=1e-06,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             rope_scaling=None,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             tie_word_embeddings=False,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             use_cache=True,\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]:             vocab_size=32000)\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]: Building model..\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]: Setting PP block ranks..\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]: Total number of parameters: 3.5G (6676.26MiB)\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]: Local number of parameters: 3.5G (6676.26MiB)\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]: [After model building] Memory usage: 40185.58MiB. Peak allocated: 40193.61MiB Peak reserved: 40202.00MiB\n",
      "03/09/2024 20:01:46 [INFO|DP=0|PP=0|TP=0]: No checkpoint path provided.\n"
     ]
    }
   ],
   "source": [
    "from nanotron.trainer import DistributedTrainer\n",
    "trainer = DistributedTrainer(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stas/openwebtext-10k'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.config.data.dataset.hf_dataset_or_datasets # Tendriamos que editar esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/mloscratch/homes/solergib/s-ai/nanotron/datasets/europarl-gpt_text_document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "@dataclasses.dataclass\n",
    "class DataCollatorForCLM:\n",
    "    \"\"\"\n",
    "    Data collator used for causal language modeling.\n",
    "\n",
    "    - input_pp_rank: Discards last input id token\n",
    "    - output_pp_rank: Discards first label id token\n",
    "    - other pp ranks: Don't have data. Instead, we use `TensorPointer` to point to the rank having the data.\n",
    "    \"\"\"\n",
    "\n",
    "    sequence_length: int\n",
    "    input_pp_rank: int\n",
    "    output_pp_rank: int\n",
    "    parallel_context: ParallelContext\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, List[np.ndarray]]]) -> Dict[str, Union[torch.Tensor, TensorPointer]]:\n",
    "        # Process the case when current rank doesn't require data. We return `TensorPointer` that points to ranks having the data.\n",
    "        current_pp_rank = dist.get_rank(self.parallel_context.pp_pg)\n",
    "        if current_pp_rank not in [\n",
    "            self.input_pp_rank,\n",
    "            self.output_pp_rank,\n",
    "        ]:\n",
    "            assert all(len(example) == 0 for example in examples)\n",
    "            return {\n",
    "                \"input_ids\": TensorPointer(group_rank=self.input_pp_rank),\n",
    "                \"input_mask\": TensorPointer(group_rank=self.input_pp_rank),\n",
    "                \"label_ids\": TensorPointer(group_rank=self.output_pp_rank),\n",
    "                \"label_mask\": TensorPointer(group_rank=self.output_pp_rank),\n",
    "            }\n",
    "\n",
    "        # Make sure we load only what's necessary, ie we only load a `input_ids` column.\n",
    "        assert all(list(example.keys()) == [\"input_ids\"] for example in examples)\n",
    "\n",
    "        # TODO @nouamanetazi: Is it better to have examples as np.array or torch.Tensor?\n",
    "        input_ids = np.vstack([examples[i][\"input_ids\"] for i in range(len(examples))])  # (b, s)\n",
    "        batch_size, expanded_input_length = input_ids.shape\n",
    "\n",
    "        result: Dict[str, Union[np.ndarray, TensorPointer]] = {}\n",
    "\n",
    "        result[\"input_ids\"] = TensorPointer(group_rank=self.input_pp_rank)\n",
    "        result[\"input_mask\"] = TensorPointer(group_rank=self.input_pp_rank)\n",
    "        result[\"label_ids\"] = TensorPointer(group_rank=self.output_pp_rank)\n",
    "        result[\"label_mask\"] = TensorPointer(group_rank=self.output_pp_rank)\n",
    "\n",
    "        assert (\n",
    "            expanded_input_length == self.sequence_length + 1\n",
    "        ), f\"Samples should be of length {self.sequence_length + 1} (seq_len+1), but got {expanded_input_length}\"\n",
    "\n",
    "        # Process inputs: last token is the label\n",
    "        if current_pp_rank == self.input_pp_rank:\n",
    "            result[\"input_ids\"] = input_ids[:, :-1]\n",
    "            result[\"input_mask\"] = np.ones((batch_size, self.sequence_length), dtype=np.bool_)\n",
    "\n",
    "        # Process labels: shift them to the left\n",
    "        if current_pp_rank == self.output_pp_rank:\n",
    "            result[\"label_ids\"] = input_ids[:, 1:]\n",
    "            result[\"label_mask\"] = np.ones((batch_size, self.sequence_length), dtype=np.bool_)\n",
    "\n",
    "        if isinstance(result[\"input_ids\"], torch.Tensor) and result[\"input_ids\"].shape[-1] != self.sequence_length:\n",
    "            raise ValueError(\n",
    "                f\"`labels` are incorrectly preprocessed. `labels` length is {result['input_ids'].shape[-1]}, but should be\"\n",
    "                f\" {self.sequence_length}.\"\n",
    "            )\n",
    "        if isinstance(result[\"label_ids\"], torch.Tensor) and result[\"label_ids\"].shape[-1] != self.sequence_length:\n",
    "            raise ValueError(\n",
    "                f\"`labels` are incorrectly preprocessed. `labels` length is {result['label_ids'].shape[-1]}, but should be\"\n",
    "                f\" {self.sequence_length}.\"\n",
    "            )\n",
    "\n",
    "        # Cast np.array to torch.Tensor\n",
    "        result = {k: v if isinstance(v, TensorPointer) else torch.from_numpy(v) for k, v in result.items()}\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanotron.parallel import ParallelContext\n",
    "from torch.utils.data import BatchSampler, DataLoader\n",
    "from nanotron import distributed as dist\n",
    "from nanotron.dataloader import EmptyInfiniteDataset\n",
    "\n",
    "def get_megatron_train_dataloader(\n",
    "    train_dataset,\n",
    "    sequence_length: int,\n",
    "    parallel_context: ParallelContext,\n",
    "    input_pp_rank: int,\n",
    "    output_pp_rank: int,\n",
    "    micro_batch_size: int,\n",
    "    consumed_train_samples: int,\n",
    "    dataloader_num_workers: int,\n",
    "    seed_worker: int,\n",
    "    dataloader_drop_last: bool = True,\n",
    "    dataloader_pin_memory: bool = True,\n",
    "    use_loop_to_round_batch_size: bool = False,\n",
    ") -> DataLoader:\n",
    "    #if not isinstance(train_dataset, datasets.Dataset): TODO: Poner bien esto\n",
    "    #    raise ValueError(f\"training requires a datasets.Dataset, but got {type(train_dataset)}\")\n",
    "\n",
    "    # Case of ranks requiring data\n",
    "    if dist.get_rank(parallel_context.pp_pg) in [\n",
    "        input_pp_rank,\n",
    "        output_pp_rank,\n",
    "    ]:\n",
    "        train_dataset = train_dataset.with_format(type=\"numpy\", columns=[\"input_ids\"], output_all_columns=True)\n",
    "\n",
    "    # Case of ranks not requiring data. We give them an infinite dummy dataloader # TODO: No es un dataloader, es un dataset, que luego con el collator se transforma. No engañeis a mi padre\n",
    "    else:\n",
    "        #\n",
    "        # TODO: Solamente se tiene que coger la length del dataset y hacer el empty \n",
    "        assert train_dataset.column_names == [\"input_ids\"], (\n",
    "            f\"Dataset has to have a single column, with `input_ids` as the column name. \"\n",
    "            f\"Current dataset: {train_dataset}\"\n",
    "        )\n",
    "        dataset_length = len(train_dataset)\n",
    "        train_dataset = train_dataset.remove_columns(column_names=\"input_ids\")\n",
    "        assert (\n",
    "            len(train_dataset) == 0\n",
    "        ), f\"Dataset has to be empty after removing the `input_ids` column. Current dataset: {train_dataset}\"\n",
    "        # HACK as if we remove the last column of a train_dataset, it becomes empty and it's number of rows becomes empty.\n",
    "        # TODO: Este hack solo quiere decir que se elimina el dataset\n",
    "        train_dataset = EmptyInfiniteDataset(length=dataset_length)\n",
    "        # No need to spawn a lot of workers, we can just use main\n",
    "        dataloader_num_workers = 0\n",
    "\n",
    "    data_collator = DataCollatorForCLM(\n",
    "        sequence_length=sequence_length,\n",
    "        input_pp_rank=input_pp_rank,\n",
    "        output_pp_rank=output_pp_rank,\n",
    "        parallel_context=parallel_context,\n",
    "    )\n",
    "\n",
    "    # Compute size and rank of dataloader workers\n",
    "    dp_ranks_size = parallel_context.dp_pg.size()\n",
    "    dp_rank = parallel_context.dp_pg.rank()\n",
    "\n",
    "    # TODO @nouamanetazi: Remove unused columns: https://github.com/huggingface/transformers/blob/47e1676255e5dd86b9541f734cd4f4bdcbb50f4a/src/transformers/trainer.py#L852\n",
    "    # TODO @nouamanetazi: Support torch.utils.data.IterableDataset: https://github.com/huggingface/transformers/blob/47e1676255e5dd86b9541f734cd4f4bdcbb50f4a/src/transformers/trainer.py#L855-L872\n",
    "\n",
    "    train_sampler = _get_train_sampler(\n",
    "        train_dataset=train_dataset,\n",
    "        dl_ranks_size=dp_ranks_size,\n",
    "        dl_rank=dp_rank,\n",
    "        seed=seed_worker,\n",
    "        use_loop_to_round_batch_size=use_loop_to_round_batch_size,\n",
    "        micro_batch_size=micro_batch_size,\n",
    "        drop_last=dataloader_drop_last,\n",
    "        consumed_train_samples=consumed_train_samples,\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=micro_batch_size,\n",
    "        sampler=train_sampler,\n",
    "        collate_fn=data_collator,\n",
    "        drop_last=dataloader_drop_last,  # we also drop_last in `clm_process()`\n",
    "        num_workers=dataloader_num_workers,\n",
    "        pin_memory=dataloader_pin_memory,\n",
    "        worker_init_fn=get_dataloader_worker_init(dp_rank=dp_rank),\n",
    "        # TODO @thomasw21: I'm not sure but this doesn't seem to work at all.\n",
    "        # pin_memory_device=\"cuda\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OG Dataloader\n",
    "def get_dataloader(trainer: DistributedTrainer, meg_data_path):\n",
    "    \"\"\"Returns a dataloader for training.\"\"\"\n",
    "\n",
    "    # First, we need to know which ranks to feed the dataloader to\n",
    "    input_pp_rank, output_pp_rank = get_input_output_pp_ranks(model=trainer.model)\n",
    "\n",
    "    # Case 3: Megatron datasets\n",
    "    if isinstance(trainer.config.data.dataset, PretrainDatasetsArgs):\n",
    "        log_rank(\"Using `megatron!!!`\", logger=logger, level=logging.INFO, rank=0)\n",
    "\n",
    "        \n",
    "\n",
    "        # We load the processed dataset on the ranks requiring it\n",
    "        dataloader = get_train_dataloader(\n",
    "            train_dataset=train_dataset,\n",
    "            sequence_length=trainer.sequence_length,\n",
    "            parallel_context=trainer.parallel_context,\n",
    "            input_pp_rank=input_pp_rank,\n",
    "            output_pp_rank=output_pp_rank,\n",
    "            micro_batch_size=trainer.micro_batch_size,\n",
    "            consumed_train_samples=trainer.consumed_train_samples,\n",
    "            dataloader_num_workers=trainer.config.data.num_loading_workers,\n",
    "            seed_worker=trainer.config.data.seed,\n",
    "            dataloader_drop_last=True,\n",
    "        )\n",
    "        # Check if we have enough samples for train_steps\n",
    "        total_tokens_dataset = len(dataloader.dataset) * trainer.sequence_length\n",
    "        num_tokens_needed_for_training = (\n",
    "            (trainer.config.tokens.train_steps - trainer.start_iteration_step)\n",
    "            * trainer.global_batch_size\n",
    "            * trainer.sequence_length\n",
    "        )\n",
    "        assert num_tokens_needed_for_training <= total_tokens_dataset, (\n",
    "            f\"Dataset is too small for steps ({total_tokens_dataset} < {num_tokens_needed_for_training}), \"\n",
    "            f\"Try train_steps<={len(dataloader.dataset) // trainer.global_batch_size + trainer.start_iteration_step}\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unhandled case of `self.config.data.dataset`. Got: {trainer.config.data.dataset}\")\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "dataloader = get_dataloader(trainer, data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
